# ==============================================================================
# destroy.yml ‚Äî Tear Down All Golden Path Infrastructure
#
# Trigger: Manual only (workflow_dispatch) ‚Äî can NEVER be triggered by a push.
#
# Inputs:
#   target_region   ‚Äî The region where the resources to DESTROY live
#   confirm_destroy ‚Äî Must type "destroy" to proceed (safety gate)
#
# IMPORTANT ‚Äî Two regions in play:
#   secrets.AWS_REGION  ‚Üí where your STATE BUCKET lives (always ap-southeast-1)
#   target_region input ‚Üí where the RESOURCES TO DESTROY live (pick us-east-1
#                         to clean up accidental deploys, or ap-southeast-1 for
#                         your real environment)
#
# Required GitHub Secrets:
#   AWS_ROLE_ARN      ‚Äî OIDC-federated IAM role
#   AWS_REGION        ‚Äî Region where your state bucket lives (ap-southeast-1)
#   TF_STATE_BUCKET   ‚Äî e.g. golden-path-tfstate-825566110381
#   TF_STATE_DYNAMODB ‚Äî e.g. golden-path-terraform-locks
# ==============================================================================

name: Golden Path ‚Äî DESTROY ALL RESOURCES üí£

on:
  workflow_dispatch:
    inputs:
      target_region:
        description: "Region where the resources to DESTROY are deployed"
        required: true
        type: choice
        options:
          - ap-southeast-1
          - us-east-1
          - us-west-2
          - eu-west-1
          - ap-northeast-1
          - ap-southeast-2

      confirm_destroy:
        description: 'Type "destroy" to confirm ‚Äî this is irreversible'
        required: true
        type: string

      destroy_state_bucket:
        description: "Also delete the Terraform state S3 bucket? (WARNING: permanent)"
        required: true
        type: boolean
        default: false

      project_name:
        description: "Project name prefix (default: golden-path)"
        required: false
        type: string
        default: "golden-path"

      environment:
        description: "Environment to destroy (default: dev)"
        required: false
        type: string
        default: "dev"

permissions:
  id-token: write
  contents: read

jobs:
  # ============================================================================
  # Safety Gate
  # ============================================================================
  confirm:
    name: ‚úã Safety Check
    runs-on: ubuntu-latest
    steps:
      - name: Verify confirmation input
        run: |
          if [ "${{ github.event.inputs.confirm_destroy }}" != "destroy" ]; then
            echo "‚ùå Confirmation failed."
            echo "   You typed: '${{ github.event.inputs.confirm_destroy }}'"
            echo "   Expected:  'destroy'"
            exit 1
          fi
          echo "‚úÖ Confirmed. Target region: ${{ github.event.inputs.target_region }}"
          echo "   State bucket region:  ${{ secrets.AWS_REGION }}"

  # ============================================================================
  # Terraform Destroy
  # ============================================================================
  terraform-destroy:
    name: üèóÔ∏è Terraform Destroy
    runs-on: ubuntu-latest
    needs: confirm
    env:
      # TARGET_REGION  ‚Äî where the resources being destroyed are
      TARGET_REGION: ${{ github.event.inputs.target_region }}
      # STATE_REGION   ‚Äî where the state bucket lives (never changes)
      STATE_REGION:  ${{ secrets.AWS_REGION }}
      PROJECT_NAME:  ${{ github.event.inputs.project_name }}
      ENVIRONMENT:   ${{ github.event.inputs.environment }}
      NAME_PREFIX:   ${{ github.event.inputs.project_name }}-${{ github.event.inputs.environment }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ github.event.inputs.target_region }}
          role-session-name: GitHubActions-Destroy

      - name: Print region summary
        run: |
          echo "=============================================="
          echo "  Target region (destroying):  ${{ env.TARGET_REGION }}"
          echo "  State bucket region:         ${{ env.STATE_REGION }}"
          echo "  State bucket:                ${{ secrets.TF_STATE_BUCKET }}"
          echo "=============================================="

      # --------------------------------------------------------------------------
      # Scale down ECS service first so tasks drain before terraform destroy
      # --------------------------------------------------------------------------
      - name: Scale down ECS service to zero
        continue-on-error: true
        run: |
          CLUSTER="${{ env.NAME_PREFIX }}-cluster"
          SERVICE="${{ env.NAME_PREFIX }}-hello-world-svc"

          EXISTS=$(aws ecs describe-services \
            --cluster "$CLUSTER" \
            --services "$SERVICE" \
            --region "${{ env.TARGET_REGION }}" \
            --query "services[?status=='ACTIVE'] | length(@)" \
            --output text 2>/dev/null || echo "0")

          if [ "$EXISTS" = "1" ]; then
            echo "Scaling ECS service to 0..."
            aws ecs update-service \
              --cluster "$CLUSTER" \
              --service "$SERVICE" \
              --desired-count 0 \
              --region "${{ env.TARGET_REGION }}"

            aws ecs wait services-stable \
              --cluster "$CLUSTER" \
              --services "$SERVICE" \
              --region "${{ env.TARGET_REGION }}" || true
            echo "‚úÖ ECS service scaled to zero"
          else
            echo "ECS service not found ‚Äî skipping scale-down"
          fi

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.7.5"

      # --------------------------------------------------------------------------
      # terraform init:
      #   backend region  ‚Üí STATE_REGION (where the .tfstate file lives)
      #   -var aws_region ‚Üí TARGET_REGION (tells Terraform which region to destroy)
      # --------------------------------------------------------------------------
      - name: Terraform Init
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=${{ env.PROJECT_NAME }}/${{ env.ENVIRONMENT }}/terraform.tfstate" \
            -backend-config="region=${{ env.STATE_REGION }}" \
            -backend-config="dynamodb_table=${{ secrets.TF_STATE_DYNAMODB }}" \
            -backend-config="encrypt=true"

      - name: Terraform Destroy
        run: |
          terraform destroy \
            -auto-approve \
            -var="aws_region=${{ env.TARGET_REGION }}" \
            -var="project_name=${{ env.PROJECT_NAME }}" \
            -var="environment=${{ env.ENVIRONMENT }}"

  # ============================================================================
  # Cleanup Leftover Resources
  # Catches anything terraform destroy misses
  # ============================================================================
  cleanup-leftovers:
    name: üßπ Clean Up Leftover Resources
    runs-on: ubuntu-latest
    needs: terraform-destroy
    if: always()
    env:
      TARGET_REGION: ${{ github.event.inputs.target_region }}
      NAME_PREFIX:   ${{ github.event.inputs.project_name }}-${{ github.event.inputs.environment }}
      PROJECT_NAME:  ${{ github.event.inputs.project_name }}
      ENVIRONMENT:   ${{ github.event.inputs.environment }}

    steps:
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ github.event.inputs.target_region }}
          role-session-name: GitHubActions-Cleanup

      - name: Force-delete Secrets Manager secret
        continue-on-error: true
        run: |
          SECRET_NAME="${{ env.PROJECT_NAME }}/${{ env.ENVIRONMENT }}/db-password"
          aws secretsmanager delete-secret \
            --secret-id "$SECRET_NAME" \
            --force-delete-without-recovery \
            --region "${{ env.TARGET_REGION }}" 2>/dev/null \
            && echo "‚úÖ Secret deleted" \
            || echo "‚ö†Ô∏è  Secret not found or already deleted"

      - name: Delete CloudWatch Log Groups
        continue-on-error: true
        run: |
          LOG_GROUPS=(
            "/ecs/${{ env.NAME_PREFIX }}/hello-world"
            "/aws/ecs/containerinsights/${{ env.NAME_PREFIX }}-cluster/performance"
          )
          for LG in "${LOG_GROUPS[@]}"; do
            aws logs delete-log-group \
              --log-group-name "$LG" \
              --region "${{ env.TARGET_REGION }}" 2>/dev/null \
              && echo "‚úÖ Deleted $LG" \
              || echo "‚ö†Ô∏è  Not found: $LG"
          done

      - name: Delete ECR images and repository
        continue-on-error: true
        run: |
          REPO="${{ env.NAME_PREFIX }}-hello-world"
          aws ecr describe-repositories \
            --repository-names "$REPO" \
            --region "${{ env.TARGET_REGION }}" 2>/dev/null || { echo "‚ö†Ô∏è  ECR repo not found"; exit 0; }

          IMAGE_IDS=$(aws ecr list-images \
            --repository-name "$REPO" \
            --region "${{ env.TARGET_REGION }}" \
            --query "imageIds[*]" \
            --output json)

          if [ "$IMAGE_IDS" != "[]" ]; then
            aws ecr batch-delete-image \
              --repository-name "$REPO" \
              --image-ids "$IMAGE_IDS" \
              --region "${{ env.TARGET_REGION }}"
          fi

          aws ecr delete-repository \
            --repository-name "$REPO" \
            --force \
            --region "${{ env.TARGET_REGION }}" 2>/dev/null \
            && echo "‚úÖ ECR repository deleted" \
            || echo "‚ö†Ô∏è  ECR repository not found"

      - name: Empty and delete app data S3 bucket
        continue-on-error: true
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="${{ env.NAME_PREFIX }}-app-data-${ACCOUNT_ID}"

          aws s3api head-bucket --bucket "$BUCKET" \
            --region "${{ env.TARGET_REGION }}" 2>/dev/null || { echo "‚ö†Ô∏è  Bucket not found"; exit 0; }

          # Delete all versioned objects
          aws s3api list-object-versions \
            --bucket "$BUCKET" \
            --region "${{ env.TARGET_REGION }}" \
            --output json \
            --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
          2>/dev/null | python3 -c "
          import sys, json, subprocess
          data = json.load(sys.stdin)
          objects = data.get('Objects') or []
          if objects:
              payload = json.dumps({'Objects': objects, 'Quiet': True})
              subprocess.run(['aws', 's3api', 'delete-objects',
                              '--bucket', '$BUCKET',
                              '--region', '${{ env.TARGET_REGION }}',
                              '--delete', payload], check=True)
              print(f'Deleted {len(objects)} versions')
          " 2>/dev/null || true

          aws s3 rm "s3://$BUCKET" --recursive \
            --region "${{ env.TARGET_REGION }}" 2>/dev/null || true

          aws s3api delete-bucket \
            --bucket "$BUCKET" \
            --region "${{ env.TARGET_REGION }}" \
            && echo "‚úÖ App data bucket deleted" \
            || echo "‚ö†Ô∏è  Could not delete bucket"

      - name: Release lingering NAT Gateway EIPs
        continue-on-error: true
        run: |
          EIP_ALLOCS=$(aws ec2 describe-addresses \
            --filters "Name=tag:Name,Values=${{ env.NAME_PREFIX }}-nat-eip" \
            --region "${{ env.TARGET_REGION }}" \
            --query "Addresses[*].AllocationId" \
            --output text 2>/dev/null)

          for ALLOC in $EIP_ALLOCS; do
            aws ec2 release-address \
              --allocation-id "$ALLOC" \
              --region "${{ env.TARGET_REGION }}" \
              && echo "‚úÖ Released EIP: $ALLOC" || true
          done

      - name: Print destroy summary
        if: always()
        run: |
          echo "=============================================="
          echo "  DESTROY SUMMARY"
          echo "  Target region:  ${{ env.TARGET_REGION }}"
          echo "  Project:        ${{ env.PROJECT_NAME }}-${{ env.ENVIRONMENT }}"
          echo "  Triggered by:   ${{ github.actor }}"
          echo "  Time:           $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "=============================================="
          echo "  Verify cleanup in AWS Console:"
          echo "  https://console.aws.amazon.com/vpc/home?region=${{ env.TARGET_REGION }}"
          echo "=============================================="

  # ============================================================================
  # Optional: Delete the Terraform State Bucket
  # Only runs if destroy_state_bucket checkbox is checked
  # ============================================================================
  destroy-state-bucket:
    name: ‚ò¢Ô∏è Delete State Bucket
    runs-on: ubuntu-latest
    needs: cleanup-leftovers
    if: ${{ github.event.inputs.destroy_state_bucket == 'true' }}
    env:
      STATE_REGION: ${{ secrets.AWS_REGION }}

    steps:
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ secrets.AWS_REGION }}
          role-session-name: GitHubActions-DestroyStateBucket

      - name: Empty and delete Terraform state bucket
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="${{ github.event.inputs.project_name }}-tfstate-${ACCOUNT_ID}"

          echo "‚ö†Ô∏è  Deleting state bucket: $BUCKET (region: ${{ env.STATE_REGION }})"

          aws s3api head-bucket --bucket "$BUCKET" \
            --region "${{ env.STATE_REGION }}" 2>/dev/null || {
            echo "Bucket not found ‚Äî already deleted"
            exit 0
          }

          aws s3api list-object-versions \
            --bucket "$BUCKET" \
            --region "${{ env.STATE_REGION }}" \
            --output json \
            --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
          | python3 -c "
          import sys, json, subprocess
          data = json.load(sys.stdin)
          objects = data.get('Objects') or []
          if objects:
              payload = json.dumps({'Objects': objects, 'Quiet': True})
              subprocess.run(['aws', 's3api', 'delete-objects',
                              '--bucket', '$BUCKET',
                              '--region', '${{ env.STATE_REGION }}',
                              '--delete', payload], check=True)
              print(f'Deleted {len(objects)} state file versions')
          "

          aws s3 rm "s3://$BUCKET" --recursive \
            --region "${{ env.STATE_REGION }}" 2>/dev/null || true

          aws s3api delete-bucket \
            --bucket "$BUCKET" \
            --region "${{ env.STATE_REGION }}" \
            && echo "‚úÖ State bucket deleted" \
            || echo "‚ùå Failed to delete state bucket"