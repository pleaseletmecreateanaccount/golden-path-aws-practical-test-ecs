# ==============================================================================
# destroy.yml ‚Äî Tear Down All Golden Path Infrastructure
#
# Trigger: Manual only (workflow_dispatch)
#
# Two destroy modes depending on the target_region input:
#
#   target_region == secrets.AWS_REGION (e.g. ap-southeast-1)
#     ‚Üí Uses terraform destroy (resources are tracked in state)
#
#   target_region != secrets.AWS_REGION (e.g. us-east-1 accidental deploy)
#     ‚Üí Skips Terraform entirely, deletes resources directly via AWS CLI
#       because those resources were never written to the state file
#
# Required GitHub Secrets:
#   AWS_ROLE_ARN      ‚Äî OIDC-federated IAM role
#   AWS_REGION        ‚Äî Region where your state bucket AND real infra live
#   TF_STATE_BUCKET   ‚Äî e.g. golden-path-tfstate-825566110381
#   TF_STATE_DYNAMODB ‚Äî e.g. golden-path-terraform-locks
# ==============================================================================

name: Golden Path ‚Äî DESTROY ALL RESOURCES üí£

on:
  workflow_dispatch:
    inputs:
      target_region:
        description: "Region where resources to DELETE are deployed"
        required: true
        type: choice
        options:
          - ap-southeast-1
          - us-east-1
          - us-west-2
          - eu-west-1
          - ap-northeast-1
          - ap-southeast-2

      confirm_destroy:
        description: 'Type "destroy" to confirm ‚Äî this is irreversible'
        required: true
        type: string

      destroy_state_bucket:
        description: "Also delete the Terraform state S3 bucket? (WARNING: permanent)"
        required: true
        type: boolean
        default: false

      project_name:
        description: "Project name prefix (default: golden-path)"
        required: false
        type: string
        default: "golden-path"

      environment:
        description: "Environment (default: dev)"
        required: false
        type: string
        default: "dev"

permissions:
  id-token: write
  contents: read

jobs:
  # ============================================================================
  # Safety Gate
  # ============================================================================
  confirm:
    name: ‚úã Safety Check
    runs-on: ubuntu-latest
    steps:
      - name: Verify confirmation input
        run: |
          if [ "${{ github.event.inputs.confirm_destroy }}" != "destroy" ]; then
            echo " Confirmation failed. You typed: '${{ github.event.inputs.confirm_destroy }}'"
            exit 1
          fi

          TARGET="${{ github.event.inputs.target_region }}"
          STATE="${{ secrets.AWS_REGION }}"

          echo " Confirmation accepted"
          echo ""
          echo "  Target region (deleting from): $TARGET"
          echo "  State bucket region:           $STATE"
          echo ""
          if [ "$TARGET" = "$STATE" ]; then
            echo "  Mode: TERRAFORM DESTROY (resources tracked in state)"
          else
            echo "  Mode: DIRECT AWS CLI DELETE (orphaned resources, not in state)"
            echo "  Terraform will be skipped ‚Äî these resources have no state entry."
          fi

  # ============================================================================
  # Job A: Terraform Destroy
  # Only runs when target_region matches the state bucket region.
  # These are resources Terraform knows about and can destroy cleanly.
  # ============================================================================
  terraform-destroy:
    name: üèóÔ∏è Terraform Destroy (state-tracked resources)
    runs-on: ubuntu-latest
    needs: confirm
    # Only run terraform destroy when targeting the region that has state
    if: github.event.inputs.target_region == secrets.AWS_REGION
    env:
      TARGET_REGION: ${{ github.event.inputs.target_region }}
      STATE_REGION:  ${{ secrets.AWS_REGION }}
      PROJECT_NAME:  ${{ github.event.inputs.project_name }}
      ENVIRONMENT:   ${{ github.event.inputs.environment }}
      NAME_PREFIX:   ${{ github.event.inputs.project_name }}-${{ github.event.inputs.environment }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ env.TARGET_REGION }}
          role-session-name: GitHubActions-TerraformDestroy

      - name: Scale down ECS service to zero
        continue-on-error: true
        run: |
          CLUSTER="${{ env.NAME_PREFIX }}-cluster"
          SERVICE="${{ env.NAME_PREFIX }}-hello-world-svc"

          EXISTS=$(aws ecs describe-services \
            --cluster "$CLUSTER" \
            --services "$SERVICE" \
            --region "${{ env.TARGET_REGION }}" \
            --query "services[?status=='ACTIVE'] | length(@)" \
            --output text 2>/dev/null || echo "0")

          if [ "$EXISTS" = "1" ]; then
            echo "Scaling ECS service to 0..."
            aws ecs update-service \
              --cluster "$CLUSTER" \
              --service "$SERVICE" \
              --desired-count 0 \
              --region "${{ env.TARGET_REGION }}"
            aws ecs wait services-stable \
              --cluster "$CLUSTER" \
              --services "$SERVICE" \
              --region "${{ env.TARGET_REGION }}" || true
            echo "‚úÖ Scaled to zero"
          else
            echo "Service not found ‚Äî skipping"
          fi

      - name: Pre-destroy -- delete NAT Gateways and release EIPs
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.NAME_PREFIX }}-vpc" \
            --region "$R" \
            --query "Vpcs[0].VpcId" \
            --output text 2>/dev/null)

          if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
            echo "No VPC found -- skipping NAT pre-delete"
            exit 0
          fi
          echo "Found VPC: $VPC_ID"

          # Delete all active NAT Gateways in this VPC
          NAT_IDS=$(aws ec2 describe-nat-gateways \
            --filter "Name=vpc-id,Values=$VPC_ID" \
                     "Name=state,Values=available,pending" \
            --region "$R" \
            --query "NatGateways[*].NatGatewayId" \
            --output text 2>/dev/null)

          if [ -z "$NAT_IDS" ]; then
            echo "No active NAT Gateways found"
          else
            for NAT in $NAT_IDS; do
              echo "Deleting NAT Gateway: $NAT"
              aws ec2 delete-nat-gateway \
                --nat-gateway-id "$NAT" \
                --region "$R" > /dev/null
            done

            # CRITICAL: wait for full deletion before Terraform touches subnets/IGW
            echo "Waiting for NAT Gateways to fully delete (this takes ~2 min)..."
            for NAT in $NAT_IDS; do
              aws ec2 wait nat-gateway-deleted \
                --nat-gateway-ids "$NAT" \
                --region "$R" 2>/dev/null || sleep 90
              echo "  NAT $NAT confirmed deleted"
            done
          fi

          # Disassociate any still-mapped EIPs (this is what causes IGW detach to fail)
          echo "Disassociating mapped EIPs..."
          ASSOC_IDS=$(aws ec2 describe-addresses \
            --filters "Name=domain,Values=vpc" \
            --region "$R" \
            --query "Addresses[?AssociationId!=null].AssociationId" \
            --output text 2>/dev/null)

          for ASSOC in $ASSOC_IDS; do
            aws ec2 disassociate-address \
              --association-id "$ASSOC" \
              --region "$R" 2>/dev/null \
              && echo "  Disassociated: $ASSOC" || true
          done

          # Release all unassociated EIPs so they stop billing
          echo "Releasing unassociated EIPs..."
          EIP_ALLOCS=$(aws ec2 describe-addresses \
            --filters "Name=domain,Values=vpc" \
            --region "$R" \
            --query "Addresses[?AssociationId==null].AllocationId" \
            --output text 2>/dev/null)

          for ALLOC in $EIP_ALLOCS; do
            aws ec2 release-address \
              --allocation-id "$ALLOC" \
              --region "$R" 2>/dev/null \
              && echo "  Released EIP: $ALLOC" \
              || echo "  Could not release: $ALLOC"
          done
          echo "Pre-destroy NAT cleanup complete"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.7.5"

      - name: Terraform Init
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=${{ env.PROJECT_NAME }}/${{ env.ENVIRONMENT }}/terraform.tfstate" \
            -backend-config="region=${{ env.STATE_REGION }}" \
            -backend-config="dynamodb_table=${{ secrets.TF_STATE_DYNAMODB }}" \
            -backend-config="encrypt=true"

      - name: Terraform Destroy
        run: |
          terraform destroy \
            -auto-approve \
            -var="aws_region=${{ env.TARGET_REGION }}" \
            -var="project_name=${{ env.PROJECT_NAME }}" \
            -var="environment=${{ env.ENVIRONMENT }}"

  # ============================================================================
  # Job B: Direct AWS CLI Delete
  # Runs when target_region DIFFERS from the state bucket region.
  # These are orphaned resources that were never written to Terraform state
  # (e.g. a failed deploy in the wrong region). Terraform cannot see them,
  # so we delete them resource-by-resource using the AWS CLI directly.
  # ============================================================================
  direct-delete:
    name: üóëÔ∏è Direct AWS CLI Delete (orphaned resources)
    runs-on: ubuntu-latest
    needs: confirm
    # Only run direct delete when targeting a DIFFERENT region than state
    if: github.event.inputs.target_region != secrets.AWS_REGION
    env:
      TARGET_REGION: ${{ github.event.inputs.target_region }}
      NAME_PREFIX:   ${{ github.event.inputs.project_name }}-${{ github.event.inputs.environment }}
      PROJECT_NAME:  ${{ github.event.inputs.project_name }}
      ENVIRONMENT:   ${{ github.event.inputs.environment }}

    steps:
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ env.TARGET_REGION }}
          role-session-name: GitHubActions-DirectDelete

      - name: "1. ECS ‚Äî scale to zero + delete service + delete cluster"
        continue-on-error: true
        run: |
          CLUSTER="${{ env.NAME_PREFIX }}-cluster"
          SERVICE="${{ env.NAME_PREFIX }}-hello-world-svc"
          R="${{ env.TARGET_REGION }}"

          echo "=== ECS Service ==="
          EXISTS=$(aws ecs describe-services \
            --cluster "$CLUSTER" --services "$SERVICE" --region "$R" \
            --query "services[?status=='ACTIVE'] | length(@)" \
            --output text 2>/dev/null || echo "0")

          if [ "$EXISTS" = "1" ]; then
            echo "Scaling to 0..."
            aws ecs update-service \
              --cluster "$CLUSTER" --service "$SERVICE" \
              --desired-count 0 --region "$R"
            aws ecs wait services-stable \
              --cluster "$CLUSTER" --services "$SERVICE" --region "$R" || true

            echo "Deleting service..."
            aws ecs delete-service \
              --cluster "$CLUSTER" --service "$SERVICE" \
              --force --region "$R" \
              && echo "‚úÖ Service deleted" || echo "‚ö†Ô∏è Service not found"
          fi

          echo "=== ECS Cluster ==="
          aws ecs delete-cluster --cluster "$CLUSTER" --region "$R" \
            && echo "‚úÖ Cluster deleted" || echo "‚ö†Ô∏è Cluster not found"

      - name: "2. ALB ‚Äî delete listener, target group, load balancer"
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"
          ALB_NAME="${{ env.NAME_PREFIX }}-alb"
          TG_NAME="${{ env.NAME_PREFIX }}-tg"

          echo "=== ALB ==="
          ALB_ARN=$(aws elbv2 describe-load-balancers \
            --names "$ALB_NAME" --region "$R" \
            --query "LoadBalancers[0].LoadBalancerArn" \
            --output text 2>/dev/null || echo "")

          if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
            # Delete listeners first
            LISTENER_ARNS=$(aws elbv2 describe-listeners \
              --load-balancer-arn "$ALB_ARN" --region "$R" \
              --query "Listeners[*].ListenerArn" --output text 2>/dev/null)
            for L in $LISTENER_ARNS; do
              aws elbv2 delete-listener --listener-arn "$L" --region "$R"
            done

            aws elbv2 delete-load-balancer \
              --load-balancer-arn "$ALB_ARN" --region "$R" \
              && echo "‚úÖ ALB deleted" || echo "‚ö†Ô∏è ALB delete failed"

            # Wait for ALB to be fully gone before deleting TG
            echo "Waiting for ALB deletion..."
            sleep 15
          else
            echo "‚ö†Ô∏è ALB not found"
          fi

          echo "=== Target Group ==="
          TG_ARN=$(aws elbv2 describe-target-groups \
            --names "$TG_NAME" --region "$R" \
            --query "TargetGroups[0].TargetGroupArn" \
            --output text 2>/dev/null || echo "")

          if [ -n "$TG_ARN" ] && [ "$TG_ARN" != "None" ]; then
            aws elbv2 delete-target-group \
              --target-group-arn "$TG_ARN" --region "$R" \
              && echo "‚úÖ Target group deleted" || echo "‚ö†Ô∏è TG delete failed"
          else
            echo "‚ö†Ô∏è Target group not found"
          fi

      - name: "3. NAT Gateway + EIP (these cost money every hour)"
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"

          echo "=== NAT Gateway ==="
          NAT_IDS=$(aws ec2 describe-nat-gateways \
            --filter "Name=tag:Name,Values=${{ env.NAME_PREFIX }}-nat-gw" \
                     "Name=state,Values=available,pending" \
            --region "$R" \
            --query "NatGateways[*].NatGatewayId" \
            --output text 2>/dev/null)

          for NAT in $NAT_IDS; do
            echo "Deleting NAT Gateway: $NAT"
            aws ec2 delete-nat-gateway --nat-gateway-id "$NAT" --region "$R"
            echo "Waiting for NAT Gateway deletion..."
            aws ec2 wait nat-gateway-deleted --nat-gateway-ids "$NAT" --region "$R" || sleep 30
            echo "‚úÖ NAT Gateway deleted"
          done

          echo "=== Elastic IPs ==="
          EIP_ALLOCS=$(aws ec2 describe-addresses \
            --filters "Name=tag:Name,Values=${{ env.NAME_PREFIX }}-nat-eip" \
            --region "$R" \
            --query "Addresses[*].AllocationId" \
            --output text 2>/dev/null)

          for ALLOC in $EIP_ALLOCS; do
            aws ec2 release-address --allocation-id "$ALLOC" --region "$R" \
              && echo "‚úÖ Released EIP: $ALLOC" || echo "‚ö†Ô∏è Could not release $ALLOC"
          done

      - name: "4. VPC ‚Äî subnets, route tables, IGW, security groups, VPC"
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"

          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.NAME_PREFIX }}-vpc" \
            --region "$R" \
            --query "Vpcs[0].VpcId" \
            --output text 2>/dev/null)

          if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
            echo "‚ö†Ô∏è VPC not found ‚Äî skipping"
            exit 0
          fi
          echo "Found VPC: $VPC_ID"

          # Subnets
          echo "Deleting subnets..."
          aws ec2 describe-subnets \
            --filters "Name=vpc-id,Values=$VPC_ID" --region "$R" \
            --query "Subnets[*].SubnetId" --output text \
          | tr '\t' '\n' | while read -r SN; do
            [ -n "$SN" ] && aws ec2 delete-subnet --subnet-id "$SN" --region "$R" \
              && echo "  ‚úÖ Deleted subnet $SN"
          done

          # Internet Gateway
          IGW_ID=$(aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=$VPC_ID" --region "$R" \
            --query "InternetGateways[0].InternetGatewayId" --output text 2>/dev/null)
          if [ -n "$IGW_ID" ] && [ "$IGW_ID" != "None" ]; then
            aws ec2 detach-internet-gateway \
              --internet-gateway-id "$IGW_ID" --vpc-id "$VPC_ID" --region "$R"
            aws ec2 delete-internet-gateway \
              --internet-gateway-id "$IGW_ID" --region "$R" \
              && echo "‚úÖ IGW deleted"
          fi

          # Route tables (non-main)
          echo "Deleting route tables..."
          aws ec2 describe-route-tables \
            --filters "Name=vpc-id,Values=$VPC_ID" --region "$R" \
            --query "RouteTables[?Associations[0].Main!=\`true\`].RouteTableId" \
            --output text \
          | tr '\t' '\n' | while read -r RT; do
            [ -n "$RT" ] && aws ec2 delete-route-table --route-table-id "$RT" --region "$R" \
              && echo "  ‚úÖ Deleted route table $RT"
          done

          # Security groups (non-default)
          echo "Deleting security groups..."
          aws ec2 describe-security-groups \
            --filters "Name=vpc-id,Values=$VPC_ID" --region "$R" \
            --query "SecurityGroups[?GroupName!='default'].GroupId" \
            --output text \
          | tr '\t' '\n' | while read -r SG; do
            [ -n "$SG" ] && aws ec2 delete-security-group --group-id "$SG" --region "$R" \
              && echo "  ‚úÖ Deleted SG $SG"
          done

          # VPC itself
          aws ec2 delete-vpc --vpc-id "$VPC_ID" --region "$R" \
            && echo "‚úÖ VPC $VPC_ID deleted" \
            || echo "‚ùå VPC delete failed ‚Äî may still have attached resources"

      - name: "5. Secrets Manager, CloudWatch Logs, Secrets"
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"

          echo "=== Secrets Manager ==="
          aws secretsmanager delete-secret \
            --secret-id "${{ env.PROJECT_NAME }}/${{ env.ENVIRONMENT }}/db-password" \
            --force-delete-without-recovery \
            --region "$R" 2>/dev/null \
            && echo "‚úÖ Secret deleted" || echo "‚ö†Ô∏è Secret not found"

          echo "=== CloudWatch Log Groups ==="
          for LG in \
            "/ecs/${{ env.NAME_PREFIX }}/hello-world" \
            "/aws/ecs/containerinsights/${{ env.NAME_PREFIX }}-cluster/performance"; do
            aws logs delete-log-group --log-group-name "$LG" --region "$R" 2>/dev/null \
              && echo "‚úÖ Deleted $LG" || echo "‚ö†Ô∏è Not found: $LG"
          done

      - name: "6. IAM Roles (global ‚Äî not region-specific)"
        continue-on-error: true
        run: |
          # IAM is global so region flag is not needed here
          EXEC_ROLE="${{ env.NAME_PREFIX }}-ecs-execution-role"
          TASK_ROLE="${{ env.NAME_PREFIX }}-ecs-task-role"
          TASK_POLICY="${{ env.NAME_PREFIX }}-task-s3-policy"

          echo "=== ECS Execution Role ==="
          # Detach all managed policies first
          POLICIES=$(aws iam list-attached-role-policies \
            --role-name "$EXEC_ROLE" \
            --query "AttachedPolicies[*].PolicyArn" \
            --output text 2>/dev/null)
          for P in $POLICIES; do
            aws iam detach-role-policy --role-name "$EXEC_ROLE" --policy-arn "$P" 2>/dev/null
          done
          aws iam delete-role --role-name "$EXEC_ROLE" 2>/dev/null \
            && echo "‚úÖ Execution role deleted" || echo "‚ö†Ô∏è Not found"

          echo "=== ECS Task Role ==="
          aws iam delete-role-policy \
            --role-name "$TASK_ROLE" \
            --policy-name "$TASK_POLICY" 2>/dev/null || true
          aws iam delete-role --role-name "$TASK_ROLE" 2>/dev/null \
            && echo "‚úÖ Task role deleted" || echo "‚ö†Ô∏è Not found"

      - name: "7. App data S3 bucket"
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="${{ env.NAME_PREFIX }}-app-data-${ACCOUNT_ID}"

          aws s3api head-bucket --bucket "$BUCKET" --region "$R" 2>/dev/null || {
            echo "‚ö†Ô∏è Bucket not found ‚Äî skipping"
            exit 0
          }

          # Empty all versioned objects
          aws s3api list-object-versions \
            --bucket "$BUCKET" --region "$R" \
            --output json \
            --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
          2>/dev/null | python3 -c "
          import sys, json, subprocess
          data = json.load(sys.stdin)
          objects = data.get('Objects') or []
          if objects:
              payload = json.dumps({'Objects': objects, 'Quiet': True})
              subprocess.run(['aws','s3api','delete-objects',
                '--bucket','$BUCKET','--region','$R',
                '--delete', payload], check=True)
              print(f'Deleted {len(objects)} versions')
          " 2>/dev/null || true

          aws s3 rm "s3://$BUCKET" --recursive --region "$R" 2>/dev/null || true
          aws s3api delete-bucket --bucket "$BUCKET" --region "$R" \
            && echo "‚úÖ App data bucket deleted" || echo "‚ö†Ô∏è Bucket delete failed"

      - name: Print summary
        if: always()
        run: |
          echo "=============================================="
          echo "  DIRECT DELETE COMPLETE"
          echo "  Region cleaned: ${{ env.TARGET_REGION }}"
          echo "  Project:        ${{ env.NAME_PREFIX }}"
          echo "  Triggered by:   ${{ github.actor }}"
          echo ""
          echo "  Verify in AWS Console:"
          echo "  https://console.aws.amazon.com/vpc/home?region=${{ env.TARGET_REGION }}"
          echo "=============================================="

  # ============================================================================
  # Shared Cleanup ‚Äî runs after EITHER terraform-destroy OR direct-delete
  # ============================================================================
  cleanup-leftovers:
    name: üßπ Final Cleanup
    runs-on: ubuntu-latest
    needs: [terraform-destroy, direct-delete]
    if: always() && (needs.terraform-destroy.result == 'success' || needs.direct-delete.result == 'success')
    env:
      TARGET_REGION: ${{ github.event.inputs.target_region }}
      NAME_PREFIX:   ${{ github.event.inputs.project_name }}-${{ github.event.inputs.environment }}

    steps:
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ github.event.inputs.target_region }}
          role-session-name: GitHubActions-Cleanup

      - name: Delete ECR repository
        continue-on-error: true
        run: |
          REPO="${{ env.NAME_PREFIX }}-hello-world"
          R="${{ env.TARGET_REGION }}"

          aws ecr describe-repositories --repository-names "$REPO" --region "$R" 2>/dev/null || {
            echo "‚ö†Ô∏è ECR repo not found"; exit 0
          }
          IMAGE_IDS=$(aws ecr list-images --repository-name "$REPO" --region "$R" \
            --query "imageIds[*]" --output json)
          [ "$IMAGE_IDS" != "[]" ] && aws ecr batch-delete-image \
            --repository-name "$REPO" --image-ids "$IMAGE_IDS" --region "$R" || true
          aws ecr delete-repository --repository-name "$REPO" --force --region "$R" \
            && echo "‚úÖ ECR repo deleted" || echo "‚ö†Ô∏è Not found"

  # ============================================================================
  # Optional: Delete the state bucket (only when checkbox is checked)
  # ============================================================================
  destroy-state-bucket:
    name: ‚ò¢Ô∏è Delete State Bucket
    runs-on: ubuntu-latest
    needs: cleanup-leftovers
    if: github.event.inputs.destroy_state_bucket == 'true'
    env:
      STATE_REGION: ${{ secrets.AWS_REGION }}

    steps:
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ secrets.AWS_REGION }}
          role-session-name: GitHubActions-DestroyStateBucket

      - name: Empty and delete Terraform state bucket
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="${{ github.event.inputs.project_name }}-tfstate-${ACCOUNT_ID}"

          aws s3api head-bucket --bucket "$BUCKET" --region "${{ env.STATE_REGION }}" 2>/dev/null || {
            echo "Bucket not found"; exit 0
          }

          aws s3api list-object-versions \
            --bucket "$BUCKET" --region "${{ env.STATE_REGION }}" \
            --output json \
            --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
          | python3 -c "
          import sys, json, subprocess
          data = json.load(sys.stdin)
          objects = data.get('Objects') or []
          if objects:
              payload = json.dumps({'Objects': objects, 'Quiet': True})
              subprocess.run(['aws','s3api','delete-objects',
                '--bucket','$BUCKET','--region','${{ env.STATE_REGION }}',
                '--delete', payload], check=True)
          "

          aws s3 rm "s3://$BUCKET" --recursive \
            --region "${{ env.STATE_REGION }}" 2>/dev/null || true
          aws s3api delete-bucket --bucket "$BUCKET" \
            --region "${{ env.STATE_REGION }}" \
            && echo "‚úÖ State bucket deleted" || echo "‚ùå Failed"