# ==============================================================================
# destroy.yml -- Tear Down All Golden Path Infrastructure
#
# Trigger: Manual only (workflow_dispatch)
#
# Two destroy modes depending on the target_region input:
#
#   mode = "terraform"  -- target_region matches secrets.AWS_REGION
#     Uses terraform destroy (resources are tracked in state)
#
#   mode = "direct"     -- target_region differs from secrets.AWS_REGION
#     Skips Terraform, deletes resources directly via AWS CLI
#     because orphaned resources were never written to the state file
#
# Required GitHub Secrets:
#   AWS_ROLE_ARN      -- OIDC-federated IAM role
#   AWS_REGION        -- Region where your state bucket AND real infra live
#   TF_STATE_BUCKET   -- e.g. golden-path-tfstate-825566110381
#   TF_STATE_DYNAMODB -- e.g. golden-path-terraform-locks
# ==============================================================================

name: Golden Path -- DESTROY ALL RESOURCES

on:
  workflow_dispatch:
    inputs:
      target_region:
        description: "Region where resources to DELETE are deployed"
        required: true
        type: choice
        options:
          - ap-southeast-1
          - us-east-1
          - us-west-2
          - eu-west-1
          - ap-northeast-1
          - ap-southeast-2

      confirm_destroy:
        description: 'Type "destroy" to confirm -- this is irreversible'
        required: true
        type: string

      destroy_state_bucket:
        description: "Also delete the Terraform state S3 bucket? (WARNING: permanent)"
        required: true
        type: boolean
        default: false

      project_name:
        description: "Project name prefix (default: golden-path)"
        required: false
        type: string
        default: "golden-path"

      environment:
        description: "Environment (default: dev)"
        required: false
        type: string
        default: "dev"

permissions:
  id-token: write
  contents: read

jobs:
  # ============================================================================
  # Safety Gate + Mode Detection
  # Outputs "mode" so downstream jobs can branch without using secrets in if:
  # GitHub Actions does not allow secrets context inside job-level if conditions.
  # The workaround is to resolve the comparison here and pass it as an output.
  # ============================================================================
  confirm:
    name: Safety Check + Mode Detection
    runs-on: ubuntu-latest
    outputs:
      mode: ${{ steps.detect.outputs.mode }}

    steps:
      - name: Verify confirmation input
        run: |
          if [ "${{ github.event.inputs.confirm_destroy }}" != "destroy" ]; then
            echo "Confirmation failed. You typed: '${{ github.event.inputs.confirm_destroy }}'"
            echo "Re-run and type exactly: destroy"
            exit 1
          fi
          echo "Confirmation accepted."

      - name: Detect destroy mode
        id: detect
        run: |
          TARGET="${{ github.event.inputs.target_region }}"
          STATE="${{ secrets.AWS_REGION }}"

          if [ "$TARGET" = "$STATE" ]; then
            echo "mode=terraform" >> $GITHUB_OUTPUT
            echo ""
            echo "Mode: TERRAFORM DESTROY"
            echo "  Target region: $TARGET"
            echo "  State region:  $STATE"
            echo "  Resources are tracked in state -- Terraform will destroy them."
          else
            echo "mode=direct" >> $GITHUB_OUTPUT
            echo ""
            echo "Mode: DIRECT AWS CLI DELETE"
            echo "  Target region: $TARGET"
            echo "  State region:  $STATE"
            echo "  Orphaned resources (not in state) -- AWS CLI will delete them."
          fi

  # ============================================================================
  # Job A: Terraform Destroy
  # Runs when target_region == secrets.AWS_REGION (mode = terraform)
  # ============================================================================
  terraform-destroy:
    name: Terraform Destroy (state-tracked resources)
    runs-on: ubuntu-latest
    needs: confirm
    if: needs.confirm.outputs.mode == 'terraform'
    env:
      TARGET_REGION: ${{ github.event.inputs.target_region }}
      STATE_REGION:  ${{ secrets.AWS_REGION }}
      PROJECT_NAME:  ${{ github.event.inputs.project_name }}
      ENVIRONMENT:   ${{ github.event.inputs.environment }}
      NAME_PREFIX:   ${{ github.event.inputs.project_name }}-${{ github.event.inputs.environment }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ env.TARGET_REGION }}
          role-session-name: GitHubActions-TerraformDestroy

      - name: Scale down ECS service to zero
        continue-on-error: true
        run: |
          CLUSTER="${{ env.NAME_PREFIX }}-cluster"
          SERVICE="${{ env.NAME_PREFIX }}-hello-world-svc"
          R="${{ env.TARGET_REGION }}"

          EXISTS=$(aws ecs describe-services \
            --cluster "$CLUSTER" \
            --services "$SERVICE" \
            --region "$R" \
            --query "services[?status=='ACTIVE'] | length(@)" \
            --output text 2>/dev/null || echo "0")

          if [ "$EXISTS" = "1" ]; then
            echo "Scaling ECS service to 0..."
            aws ecs update-service \
              --cluster "$CLUSTER" \
              --service "$SERVICE" \
              --desired-count 0 \
              --region "$R"
            aws ecs wait services-stable \
              --cluster "$CLUSTER" \
              --services "$SERVICE" \
              --region "$R" || true
            echo "Scaled to zero"
          else
            echo "Service not found -- skipping"
          fi

      - name: Pre-destroy -- delete NAT Gateways and release EIPs
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"

          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.NAME_PREFIX }}-vpc" \
            --region "$R" \
            --query "Vpcs[0].VpcId" \
            --output text 2>/dev/null)

          if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
            echo "No VPC found -- skipping NAT pre-delete"
            exit 0
          fi
          echo "Found VPC: $VPC_ID"

          # Delete all active NAT Gateways in this VPC
          NAT_IDS=$(aws ec2 describe-nat-gateways \
            --filter "Name=vpc-id,Values=$VPC_ID" \
                     "Name=state,Values=available,pending" \
            --region "$R" \
            --query "NatGateways[*].NatGatewayId" \
            --output text 2>/dev/null)

          if [ -z "$NAT_IDS" ]; then
            echo "No active NAT Gateways found"
          else
            for NAT in $NAT_IDS; do
              echo "Deleting NAT Gateway: $NAT"
              aws ec2 delete-nat-gateway \
                --nat-gateway-id "$NAT" \
                --region "$R" > /dev/null
            done

            # CRITICAL: wait for full deletion before Terraform touches subnets/IGW
            echo "Waiting for NAT Gateways to fully delete (this takes ~2 min)..."
            for NAT in $NAT_IDS; do
              aws ec2 wait nat-gateway-deleted \
                --nat-gateway-ids "$NAT" \
                --region "$R" 2>/dev/null || sleep 90
              echo "  NAT $NAT confirmed deleted"
            done
          fi

          # Disassociate any still-mapped EIPs (causes IGW detach to fail)
          echo "Disassociating mapped EIPs..."
          ASSOC_IDS=$(aws ec2 describe-addresses \
            --filters "Name=domain,Values=vpc" \
            --region "$R" \
            --query "Addresses[?AssociationId!=null].AssociationId" \
            --output text 2>/dev/null)

          for ASSOC in $ASSOC_IDS; do
            aws ec2 disassociate-address \
              --association-id "$ASSOC" \
              --region "$R" 2>/dev/null \
              && echo "  Disassociated: $ASSOC" || true
          done

          # Release all unassociated EIPs
          echo "Releasing unassociated EIPs..."
          EIP_ALLOCS=$(aws ec2 describe-addresses \
            --filters "Name=domain,Values=vpc" \
            --region "$R" \
            --query "Addresses[?AssociationId==null].AllocationId" \
            --output text 2>/dev/null)

          for ALLOC in $EIP_ALLOCS; do
            aws ec2 release-address \
              --allocation-id "$ALLOC" \
              --region "$R" 2>/dev/null \
              && echo "  Released EIP: $ALLOC" \
              || echo "  Could not release: $ALLOC"
          done
          echo "Pre-destroy NAT cleanup complete"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.7.5"

      - name: Terraform Init
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=${{ env.PROJECT_NAME }}/${{ env.ENVIRONMENT }}/terraform.tfstate" \
            -backend-config="region=${{ env.STATE_REGION }}" \
            -backend-config="dynamodb_table=${{ secrets.TF_STATE_DYNAMODB }}" \
            -backend-config="encrypt=true"

      - name: Terraform Destroy
        run: |
          terraform destroy \
            -auto-approve \
            -var="aws_region=${{ env.TARGET_REGION }}" \
            -var="project_name=${{ env.PROJECT_NAME }}" \
            -var="environment=${{ env.ENVIRONMENT }}"

  # ============================================================================
  # Job B: Direct AWS CLI Delete
  # Runs when target_region != secrets.AWS_REGION (mode = direct)
  # Orphaned resources were never written to Terraform state so Terraform
  # cannot see them -- delete each resource type directly via AWS CLI.
  # ============================================================================
  direct-delete:
    name: Direct AWS CLI Delete (orphaned resources)
    runs-on: ubuntu-latest
    needs: confirm
    if: needs.confirm.outputs.mode == 'direct'
    env:
      TARGET_REGION: ${{ github.event.inputs.target_region }}
      NAME_PREFIX:   ${{ github.event.inputs.project_name }}-${{ github.event.inputs.environment }}
      PROJECT_NAME:  ${{ github.event.inputs.project_name }}
      ENVIRONMENT:   ${{ github.event.inputs.environment }}

    steps:
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ env.TARGET_REGION }}
          role-session-name: GitHubActions-DirectDelete

      - name: "1. ECS -- scale to zero, delete service, delete cluster"
        continue-on-error: true
        run: |
          CLUSTER="${{ env.NAME_PREFIX }}-cluster"
          SERVICE="${{ env.NAME_PREFIX }}-hello-world-svc"
          R="${{ env.TARGET_REGION }}"

          EXISTS=$(aws ecs describe-services \
            --cluster "$CLUSTER" --services "$SERVICE" --region "$R" \
            --query "services[?status=='ACTIVE'] | length(@)" \
            --output text 2>/dev/null || echo "0")

          if [ "$EXISTS" = "1" ]; then
            aws ecs update-service \
              --cluster "$CLUSTER" --service "$SERVICE" \
              --desired-count 0 --region "$R"
            aws ecs wait services-stable \
              --cluster "$CLUSTER" --services "$SERVICE" --region "$R" || true
            aws ecs delete-service \
              --cluster "$CLUSTER" --service "$SERVICE" \
              --force --region "$R" \
              && echo "Service deleted" || echo "Service not found"
          fi

          aws ecs delete-cluster --cluster "$CLUSTER" --region "$R" \
            && echo "Cluster deleted" || echo "Cluster not found"

      - name: "2. ALB -- delete listeners, load balancer, target group"
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"

          ALB_ARN=$(aws elbv2 describe-load-balancers \
            --names "${{ env.NAME_PREFIX }}-alb" --region "$R" \
            --query "LoadBalancers[0].LoadBalancerArn" \
            --output text 2>/dev/null || echo "")

          if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
            LISTENERS=$(aws elbv2 describe-listeners \
              --load-balancer-arn "$ALB_ARN" --region "$R" \
              --query "Listeners[*].ListenerArn" --output text 2>/dev/null)
            for L in $LISTENERS; do
              aws elbv2 delete-listener --listener-arn "$L" --region "$R"
            done
            aws elbv2 delete-load-balancer \
              --load-balancer-arn "$ALB_ARN" --region "$R" \
              && echo "ALB deleted"
            sleep 15
          else
            echo "ALB not found"
          fi

          TG_ARN=$(aws elbv2 describe-target-groups \
            --names "${{ env.NAME_PREFIX }}-tg" --region "$R" \
            --query "TargetGroups[0].TargetGroupArn" \
            --output text 2>/dev/null || echo "")

          if [ -n "$TG_ARN" ] && [ "$TG_ARN" != "None" ]; then
            aws elbv2 delete-target-group \
              --target-group-arn "$TG_ARN" --region "$R" \
              && echo "Target group deleted"
          fi

      - name: "3. NAT Gateways + Elastic IPs (billed hourly -- delete first)"
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"

          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.NAME_PREFIX }}-vpc" \
            --region "$R" \
            --query "Vpcs[0].VpcId" --output text 2>/dev/null)

          NAT_IDS=$(aws ec2 describe-nat-gateways \
            --filter "Name=vpc-id,Values=$VPC_ID" \
                     "Name=state,Values=available,pending" \
            --region "$R" \
            --query "NatGateways[*].NatGatewayId" \
            --output text 2>/dev/null)

          for NAT in $NAT_IDS; do
            echo "Deleting NAT Gateway: $NAT"
            aws ec2 delete-nat-gateway --nat-gateway-id "$NAT" --region "$R" > /dev/null
          done

          if [ -n "$NAT_IDS" ]; then
            echo "Waiting for NAT Gateways to delete..."
            for NAT in $NAT_IDS; do
              aws ec2 wait nat-gateway-deleted \
                --nat-gateway-ids "$NAT" --region "$R" 2>/dev/null || sleep 90
              echo "  NAT $NAT deleted"
            done
          fi

          ASSOC_IDS=$(aws ec2 describe-addresses \
            --filters "Name=domain,Values=vpc" --region "$R" \
            --query "Addresses[?AssociationId!=null].AssociationId" \
            --output text 2>/dev/null)
          for ASSOC in $ASSOC_IDS; do
            aws ec2 disassociate-address --association-id "$ASSOC" --region "$R" 2>/dev/null || true
          done

          EIP_ALLOCS=$(aws ec2 describe-addresses \
            --filters "Name=domain,Values=vpc" --region "$R" \
            --query "Addresses[?AssociationId==null].AllocationId" \
            --output text 2>/dev/null)
          for ALLOC in $EIP_ALLOCS; do
            aws ec2 release-address --allocation-id "$ALLOC" --region "$R" 2>/dev/null \
              && echo "  Released EIP: $ALLOC" || true
          done

      - name: "4. VPC -- subnets, IGW, route tables, security groups, VPC"
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"

          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.NAME_PREFIX }}-vpc" \
            --region "$R" \
            --query "Vpcs[0].VpcId" --output text 2>/dev/null)

          if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
            echo "VPC not found -- skipping"; exit 0
          fi
          echo "Found VPC: $VPC_ID"

          aws ec2 describe-subnets \
            --filters "Name=vpc-id,Values=$VPC_ID" --region "$R" \
            --query "Subnets[*].SubnetId" --output text \
          | tr '\t' '\n' | while read -r SN; do
            [ -n "$SN" ] && aws ec2 delete-subnet \
              --subnet-id "$SN" --region "$R" 2>/dev/null \
              && echo "  Deleted subnet $SN"
          done

          IGW_ID=$(aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=$VPC_ID" --region "$R" \
            --query "InternetGateways[0].InternetGatewayId" --output text 2>/dev/null)
          if [ -n "$IGW_ID" ] && [ "$IGW_ID" != "None" ]; then
            aws ec2 detach-internet-gateway \
              --internet-gateway-id "$IGW_ID" --vpc-id "$VPC_ID" --region "$R"
            aws ec2 delete-internet-gateway \
              --internet-gateway-id "$IGW_ID" --region "$R" \
              && echo "IGW deleted"
          fi

          aws ec2 describe-route-tables \
            --filters "Name=vpc-id,Values=$VPC_ID" --region "$R" \
            --query "RouteTables[?Associations[0].Main!=\`true\`].RouteTableId" \
            --output text \
          | tr '\t' '\n' | while read -r RT; do
            [ -n "$RT" ] && aws ec2 delete-route-table \
              --route-table-id "$RT" --region "$R" 2>/dev/null \
              && echo "  Deleted route table $RT"
          done

          aws ec2 describe-security-groups \
            --filters "Name=vpc-id,Values=$VPC_ID" --region "$R" \
            --query "SecurityGroups[?GroupName!='default'].GroupId" \
            --output text \
          | tr '\t' '\n' | while read -r SG; do
            [ -n "$SG" ] && aws ec2 delete-security-group \
              --group-id "$SG" --region "$R" 2>/dev/null \
              && echo "  Deleted SG $SG"
          done

          aws ec2 delete-vpc --vpc-id "$VPC_ID" --region "$R" \
            && echo "VPC $VPC_ID deleted" \
            || echo "VPC delete failed -- check console for remaining dependencies"

      - name: "5. Secrets Manager + CloudWatch Log Groups"
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"

          aws secretsmanager delete-secret \
            --secret-id "${{ env.PROJECT_NAME }}/${{ env.ENVIRONMENT }}/db-password" \
            --force-delete-without-recovery \
            --region "$R" 2>/dev/null \
            && echo "Secret deleted" || echo "Secret not found"

          for LG in \
            "/ecs/${{ env.NAME_PREFIX }}/hello-world" \
            "/aws/ecs/containerinsights/${{ env.NAME_PREFIX }}-cluster/performance"; do
            aws logs delete-log-group \
              --log-group-name "$LG" --region "$R" 2>/dev/null \
              && echo "Deleted $LG" || echo "Not found: $LG"
          done

      - name: "6. IAM Roles (global)"
        continue-on-error: true
        run: |
          EXEC_ROLE="${{ env.NAME_PREFIX }}-ecs-execution-role"
          TASK_ROLE="${{ env.NAME_PREFIX }}-ecs-task-role"

          POLICIES=$(aws iam list-attached-role-policies \
            --role-name "$EXEC_ROLE" \
            --query "AttachedPolicies[*].PolicyArn" \
            --output text 2>/dev/null)
          for P in $POLICIES; do
            aws iam detach-role-policy --role-name "$EXEC_ROLE" --policy-arn "$P" 2>/dev/null
          done
          aws iam delete-role --role-name "$EXEC_ROLE" 2>/dev/null \
            && echo "Execution role deleted" || echo "Not found"

          aws iam delete-role-policy \
            --role-name "$TASK_ROLE" \
            --policy-name "${{ env.NAME_PREFIX }}-task-s3-policy" 2>/dev/null || true
          aws iam delete-role --role-name "$TASK_ROLE" 2>/dev/null \
            && echo "Task role deleted" || echo "Not found"

      - name: "7. App data S3 bucket"
        continue-on-error: true
        run: |
          R="${{ env.TARGET_REGION }}"
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="${{ env.NAME_PREFIX }}-app-data-${ACCOUNT_ID}"

          aws s3api head-bucket --bucket "$BUCKET" --region "$R" 2>/dev/null || {
            echo "Bucket not found -- skipping"; exit 0
          }

          aws s3api list-object-versions \
            --bucket "$BUCKET" --region "$R" \
            --output json \
            --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
          2>/dev/null | python3 -c "
          import sys, json, subprocess
          data = json.load(sys.stdin)
          objects = data.get('Objects') or []
          if objects:
              payload = json.dumps({'Objects': objects, 'Quiet': True})
              subprocess.run(['aws','s3api','delete-objects',
                '--bucket','$BUCKET','--region','$R',
                '--delete', payload], check=True)
              print(f'Deleted {len(objects)} versions')
          " 2>/dev/null || true

          aws s3 rm "s3://$BUCKET" --recursive --region "$R" 2>/dev/null || true
          aws s3api delete-bucket --bucket "$BUCKET" --region "$R" \
            && echo "App data bucket deleted" || echo "Bucket delete failed"

      - name: Print summary
        if: always()
        run: |
          echo "================================================"
          echo "  DIRECT DELETE COMPLETE"
          echo "  Region: ${{ env.TARGET_REGION }}"
          echo "  Prefix: ${{ env.NAME_PREFIX }}"
          echo "  Triggered by: ${{ github.actor }}"
          echo "  Verify: https://console.aws.amazon.com/vpc/home?region=${{ env.TARGET_REGION }}"
          echo "================================================"

  # ============================================================================
  # Shared Cleanup -- runs after EITHER job A or job B completes
  # ============================================================================
  cleanup-leftovers:
    name: Final Cleanup
    runs-on: ubuntu-latest
    needs: [terraform-destroy, direct-delete]
    if: always() && (needs.terraform-destroy.result == 'success' || needs.direct-delete.result == 'success' || needs.terraform-destroy.result == 'skipped' || needs.direct-delete.result == 'skipped')
    env:
      TARGET_REGION: ${{ github.event.inputs.target_region }}
      NAME_PREFIX:   ${{ github.event.inputs.project_name }}-${{ github.event.inputs.environment }}

    steps:
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ github.event.inputs.target_region }}
          role-session-name: GitHubActions-Cleanup

      - name: Delete ECR repository
        continue-on-error: true
        run: |
          REPO="${{ env.NAME_PREFIX }}-hello-world"
          R="${{ env.TARGET_REGION }}"

          aws ecr describe-repositories \
            --repository-names "$REPO" --region "$R" 2>/dev/null || {
            echo "ECR repo not found"; exit 0
          }

          IMAGE_IDS=$(aws ecr list-images \
            --repository-name "$REPO" --region "$R" \
            --query "imageIds[*]" --output json)
          [ "$IMAGE_IDS" != "[]" ] && aws ecr batch-delete-image \
            --repository-name "$REPO" \
            --image-ids "$IMAGE_IDS" \
            --region "$R" || true

          aws ecr delete-repository \
            --repository-name "$REPO" --force --region "$R" \
            && echo "ECR repo deleted" || echo "Not found"

  # ============================================================================
  # Optional: Delete the Terraform state bucket
  # ============================================================================
  destroy-state-bucket:
    name: Delete State Bucket (optional)
    runs-on: ubuntu-latest
    needs: cleanup-leftovers
    if: github.event.inputs.destroy_state_bucket == 'true'
    env:
      STATE_REGION: ${{ secrets.AWS_REGION }}

    steps:
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume:    ${{ secrets.AWS_ROLE_ARN }}
          aws-region:        ${{ secrets.AWS_REGION }}
          role-session-name: GitHubActions-DestroyStateBucket

      - name: Empty and delete Terraform state bucket
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET="${{ github.event.inputs.project_name }}-tfstate-${ACCOUNT_ID}"

          aws s3api head-bucket --bucket "$BUCKET" \
            --region "${{ env.STATE_REGION }}" 2>/dev/null || {
            echo "Bucket not found"; exit 0
          }

          aws s3api list-object-versions \
            --bucket "$BUCKET" --region "${{ env.STATE_REGION }}" \
            --output json \
            --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
          | python3 -c "
          import sys, json, subprocess
          data = json.load(sys.stdin)
          objects = data.get('Objects') or []
          if objects:
              payload = json.dumps({'Objects': objects, 'Quiet': True})
              subprocess.run(['aws','s3api','delete-objects',
                '--bucket','$BUCKET','--region','${{ env.STATE_REGION }}',
                '--delete', payload], check=True)
              print(f'Deleted {len(objects)} state versions')
          "

          aws s3 rm "s3://$BUCKET" --recursive \
            --region "${{ env.STATE_REGION }}" 2>/dev/null || true
          aws s3api delete-bucket --bucket "$BUCKET" \
            --region "${{ env.STATE_REGION }}" \
            && echo "State bucket deleted" || echo "Failed to delete state bucket"